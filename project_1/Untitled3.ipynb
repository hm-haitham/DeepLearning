{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rEcRaXSOojPX"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gV4wZt18oh5H"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "NB_SAMPLES = 1000\n",
    "DATA_DIR = './data'\n",
    "\n",
    "NUMBER_OF_CLASSES = 10\n",
    "\n",
    "WIDTH_HEIGHT = 14\n",
    "SINGLE_IMAGE_SIZE = WIDTH_HEIGHT * WIDTH_HEIGHT\n",
    "DOUBLE_IMAGE_SIZE = 2 * SINGLE_IMAGE_SIZE\n",
    "\n",
    "# ----Train Config-----#\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "SUB_CRITERION = nn.CrossEntropyLoss()\n",
    "FINAL_CRITERION = nn.BCELoss()\n",
    "EPOCHS = 20\n",
    "\n",
    "TRAIN_CHECKPOINTS_DIR = \"./checkpoints\"\n",
    "SAVE_MODEL_EVERY_X_EPOCH = 5\n",
    "\n",
    "# ----AuxLoss Config-----#\n",
    "ALPHA = 0.5\n",
    "BETA = 0.25\n",
    "GAMMA = 0.25\n",
    "WEIGHTS_LOSS = ALPHA, BETA, GAMMA\n",
    "\n",
    "#----Test Config-----#\n",
    "TEST_BATCH_SIZE = NB_SAMPLES\n",
    "\n",
    "#----BasicNet Config-----#\n",
    "BASIC_NET_NAME = \"basic_net\"\n",
    "BASIC_NET_HIDDEN_LAYER = 512\n",
    "\n",
    "#----OscarNet Config-----#\n",
    "OSCAR_NET_NAME = \"oscar_net\"\n",
    "OSCAR_NET_HIDDEN_LAYER = 512\n",
    "\n",
    "#----DesmondNet Config-----#\n",
    "DESMOND_NET_NAME = \"desmond_net\"\n",
    "DESMOND_NET_HIDDEN_LAYER = 512\n",
    "\n",
    "#----RobertNet Config-----#\n",
    "ROBERT_NET_NAME = \"robert_net\"\n",
    "ROBERT_NET_HIDDEN_LAYER = 512\n",
    "ROBERT_NET_BASE_CHANNEL_SIZE = 8\n",
    "\n",
    "#----LeonardtNet Config-----#\n",
    "LEONARD_NET_NAME = \"leonard_net\"\n",
    "LEONARD_NET_HIDDEN_LAYER = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qps5BVrJnC81"
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEH4NF5zl6vd"
   },
   "outputs": [],
   "source": [
    "############################################# HELPERS ###############################\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# The data\n",
    "\n",
    "def convert_to_one_hot_labels(input, target):\n",
    "    tmp = input.new_zeros(target.size(0), target.max() + 1)\n",
    "    #set ones\n",
    "    tmp.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return tmp\n",
    "\n",
    "def load_data(cifar = None, one_hot_labels = False, normalize = False, flatten = True):\n",
    "\n",
    "    data_dir = './data'\n",
    "\n",
    "    if (cifar is not None and cifar):\n",
    "        print('* Using CIFAR')\n",
    "        cifar_train_set = datasets.CIFAR10(data_dir + '/cifar10/', train = True, download = True)\n",
    "        cifar_test_set = datasets.CIFAR10(data_dir + '/cifar10/', train = False, download = True)\n",
    "\n",
    "        train_input = torch.from_numpy(cifar_train_set.data)\n",
    "        train_input = train_input.transpose(3, 1).transpose(2, 3).float()\n",
    "        train_target = torch.tensor(cifar_train_set.targets, dtype = torch.int64)\n",
    "\n",
    "        test_input = torch.from_numpy(cifar_test_set.data).float()\n",
    "        test_input = test_input.transpose(3, 1).transpose(2, 3).float()\n",
    "        test_target = torch.tensor(cifar_test_set.targets, dtype = torch.int64)\n",
    "\n",
    "    else:\n",
    "        print('* Using MNIST')\n",
    "        mnist_train_set = datasets.MNIST(data_dir + '/mnist/', train = True, download = True)\n",
    "        mnist_test_set = datasets.MNIST(data_dir + '/mnist/', train = False, download = True)\n",
    "\n",
    "        train_input = mnist_train_set.data.view(-1, 1, 28, 28).float()\n",
    "        train_target = mnist_train_set.targets\n",
    "        test_input = mnist_test_set.data.view(-1, 1, 28, 28).float()\n",
    "        test_target = mnist_test_set.targets\n",
    "\n",
    "    if flatten:\n",
    "        train_input = train_input.clone().reshape(train_input.size(0), -1)\n",
    "        test_input = test_input.clone().reshape(test_input.size(0), -1)\n",
    "        \n",
    "        \n",
    "    train_input = train_input.narrow(0, 0, 1000)\n",
    "    train_target = train_target.narrow(0, 0, 1000)\n",
    "    test_input = test_input.narrow(0, 0, 1000)\n",
    "    test_target = test_target.narrow(0, 0, 1000)\n",
    "\n",
    "    print('** Use {:d} train and {:d} test samples'.format(train_input.size(0), test_input.size(0)))\n",
    "\n",
    "    if one_hot_labels:\n",
    "        train_target = convert_to_one_hot_labels(train_input, train_target)\n",
    "        test_target = convert_to_one_hot_labels(test_input, test_target)\n",
    "\n",
    "    if normalize:\n",
    "        mu, std = train_input.mean(), train_input.std()\n",
    "        train_input.sub_(mu).div_(std)\n",
    "        test_input.sub_(mu).div_(std)\n",
    "\n",
    "    return train_input, train_target, test_input, test_target\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def mnist_to_pairs(nb, input, target):\n",
    "    input = torch.functional.F.avg_pool2d(input, kernel_size = 2)\n",
    "    a = torch.randperm(input.size(0))\n",
    "    a = a[:2 * nb].view(nb, 2)\n",
    "    input = torch.cat((input[a[:, 0]], input[a[:, 1]]), 1)\n",
    "    classes = target[a]\n",
    "    target = (classes[:, 0] <= classes[:, 1]).long()\n",
    "    return input, target, classes\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def generate_pair_sets(nb):\n",
    "\n",
    "    data_dir = DATA_DIR\n",
    "\n",
    "    train_set = datasets.MNIST(data_dir + '/mnist/', train = True, download = True)\n",
    "    train_input = train_set.data.view(-1, 1, 28, 28).float()\n",
    "    train_target = train_set.targets\n",
    "\n",
    "    test_set = datasets.MNIST(data_dir + '/mnist/', train = False, download = True)\n",
    "    test_input = test_set.data.view(-1, 1, 28, 28).float()\n",
    "    test_target = test_set.targets\n",
    "\n",
    "    return mnist_to_pairs(nb, train_input, train_target) + \\\n",
    "           mnist_to_pairs(nb, test_input, test_target)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def compute_accuracy(tensor1, tensor2):\n",
    "    \n",
    "    tensor_accuracy = torch.where(tensor1 == tensor2, torch.tensor(1), torch.tensor(0))\n",
    "    \n",
    "    accuracy = torch.sum(tensor_accuracy).item() / NB_SAMPLES\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def save_model(model, epoch=None, loss=None, save_dir=None, specific_name=None):\n",
    "\n",
    "    if epoch and loss and save_dir and specific_name:\n",
    "        model_name = model.model_name\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        file_name = f\"{timestr}_{model_name}_epoch_{epoch}_loss_{loss:03.3f}.pt\"\n",
    "        Path(save_dir).mkdir(exist_ok=True)\n",
    "        file_path = Path(save_dir) / file_name\n",
    "        torch.save(model.state_dict(), str(file_path))\n",
    "    elif save_dir and specific_name:\n",
    "        file_path = Path(save_dir) / specific_name\n",
    "        torch.save(model.state_dict(), str(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hcs0jYL1nJ9g"
   },
   "source": [
    "# Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_urcl61NmzfS"
   },
   "outputs": [],
   "source": [
    "###############################3 SIAMESE ###########################################3\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "class siamese_net(nn.Module):\n",
    "\n",
    "    def __init__(self, weight_sharing = True, architecture = 1, nb_channels = 24, nb_hidden = 300, k = 1):\n",
    "        \n",
    "        super(siamese_net, self).__init__()\n",
    "        \n",
    "        #if weight sharing forward on the same subnet\n",
    "        if weight_sharing: \n",
    "            self.nb_subnet = 1\n",
    "        else:\n",
    "            self.nb_subnet = 2\n",
    "        \n",
    "        if (architecture == 1 ):\n",
    "            # subnetwork 1\n",
    "            # (1,14,14) conv (1,3,3) =>  (1,12,12)\n",
    "            # Relu \n",
    "            # (1,12,12) conv (1,3,3) => (1,10,10)\n",
    "            # Relu\n",
    "            # (1,10,10) maxpool (2,2) => (5,5)\n",
    "            # try to add different channels and add dropout ? \n",
    "            self.conv = nn.ModuleList([nn.Sequential(nn.Conv2d(1, nb_channels , kernel_size=3),  \n",
    "                                   nn.LeakyReLU(),\n",
    "                                   nn.Dropout(p=0.2),                  \n",
    "                                   nn.Conv2d(nb_channels, 2 * nb_channels, kernel_size=3),\n",
    "                                   nn.LeakyReLU(),           \n",
    "                                   nn.MaxPool2d(2),\n",
    "                                   nn.Dropout(p=0.2)) for i in range(self.nb_subnet) ] )\n",
    "        \n",
    "            #fully connected part   \n",
    "            # from a conv (5,5)  => fc layer (25,300) => Relu => fc layer (300,10) \n",
    "            \n",
    "            out_conv = 25 * 2 * nb_channels                                          \n",
    "            \n",
    "            self.fc = nn.ModuleList([nn.Sequential(nn.Linear(out_conv , nb_hidden), nn.LeakyReLU(), nn.Dropout(p=0.2),\n",
    "                                                   nn.Linear(nb_hidden, 10) ) for i in range(self.nb_subnet) ] )\n",
    "        if (architecture == 2 ):\n",
    "            #(W−F+2P)/S+1\n",
    "            \n",
    "            self.conv = nn.ModuleList([nn.Sequential(nn.Conv2d(1, nb_channels, kernel_size=5, stride=1, padding=1), #(14 - 5 + 2 ) + 1 =12\n",
    "                                                     nn.LeakyReLU(),\n",
    "                                                     nn.MaxPool2d(kernel_size=3, stride=3), #4\n",
    "                                                     nn.Dropout(p=0.2),\n",
    "                                                     nn.Conv2d(nb_channels, 2 * nb_channels, \n",
    "                                                               kernel_size=3, stride=1, padding=1), #(4-3 +2) + 1 = 4\n",
    "                                                     nn.LeakyReLU(),\n",
    "                                                     nn.Dropout(p=0.2)) for i in range(self.nb_subnet) ] )\n",
    "            \n",
    "            out_conv = 4 * 4 * 2 * nb_channels #flatten the (2,2, 2 * nb_channels) tensor                                         \n",
    "            \n",
    "            self.fc = nn.ModuleList([nn.Sequential(nn.Linear(out_conv , nb_hidden), nn.LeakyReLU(), nn.Dropout(p=0.2),\n",
    "                                                   nn.Linear(nb_hidden, 10) ) for i in range(self.nb_subnet) ] )\n",
    "\n",
    "        if (architecture == 3 ):\n",
    "            \n",
    "            #(W−F+2P)/S+1\n",
    "           \n",
    "            self.conv = nn.ModuleList([nn.Sequential(nn.Conv2d(1, nb_channels, kernel_size=3, stride=1, padding=0),  #(14-3+0) + 1 = 12 \n",
    "                                                     nn.LeakyReLU(),\n",
    "                                                     nn.MaxPool2d(kernel_size=3, stride=3), #4\n",
    "                                                     nn.Dropout(p=0.2),\n",
    "                                                     nn.Conv2d(nb_channels, 2 * nb_channels, kernel_size=2, stride=1, padding=0), #3\n",
    "                                                     nn.LeakyReLU(),\n",
    "                                                     nn.Dropout(p=0.2)) for i in range(self.nb_subnet) ] )\n",
    "                                                     \n",
    "            out_conv = 3* 3 * 2 * nb_channels                                          \n",
    "            \n",
    "            self.fc = nn.ModuleList([nn.Sequential(nn.Linear(out_conv , nb_hidden), nn.LeakyReLU(), nn.Dropout(p=0.2),\n",
    "                                                   nn.Linear(nb_hidden, 10) ) for i in range(self.nb_subnet) ] )\n",
    "        \n",
    "        if (architecture == 4 ):\n",
    "            \n",
    "            #(W−F+2P)/S+1\n",
    "            self.conv = nn.ModuleList([nn.Sequential(nn.Conv2d(1, nb_channels, kernel_size=2, stride=1, padding=1),  #(14-2+2)+1 = 15\n",
    "                                                     nn.LeakyReLU(),\n",
    "                                                     nn.MaxPool2d(kernel_size=3, stride=3), #5\n",
    "                                                     nn.Dropout(p=0.2),\n",
    "                                                     nn.Conv2d(nb_channels, 2 * nb_channels, \n",
    "                                                               kernel_size=3, stride=1, padding=0), #(5-3)+1 = 3\n",
    "                                                     nn.LeakyReLU(),\n",
    "                                                     #nn.MaxPool2d(kernel_size=3, stride=3), #1  \n",
    "                                                     nn.Dropout(p=0.2) ) for i in range(self.nb_subnet) ] )\n",
    "            \n",
    "            out_conv = 3* 3 * 2 * nb_channels \n",
    "            \n",
    "            self.fc = nn.ModuleList([nn.Sequential(nn.Linear(out_conv , nb_hidden), nn.LeakyReLU(), nn.Dropout(p=0.2),\n",
    "                                                   nn.Linear(nb_hidden, 10) ) for i in range(self.nb_subnet) ] )\n",
    "        if (architecture == 5 ):\n",
    "            \n",
    "            #(W−F+2P)/S+1\n",
    "            #k= 1, 3, 5  \n",
    "            self.conv = nn.ModuleList([nn.Sequential(nn.Conv2d(1, nb_channels, kernel_size= k ),  #(14-k)+1 = 15 -k # 14, 12,10\n",
    "                                                     nn.LeakyReLU(),\n",
    "                                                     nn.MaxPool2d(kernel_size=2, stride=2), #(15-k) / 2 : 7, 6, 5  \n",
    "                                                     nn.Dropout(p=0.2),\n",
    "                                                     nn.Conv2d(nb_channels, 2 * nb_channels, \n",
    "                                                               kernel_size = k),    #7, 5, 3   # (15 - k) /2 - k + 1\n",
    "                                                     nn.LeakyReLU(),\n",
    "                                                     #nn.MaxPool2d(kernel_size=3, stride=3), #1  \n",
    "                                                     nn.Dropout(p=0.2) ) for i in range(self.nb_subnet) ] )\n",
    "           \n",
    "            out_conv = int( ( ((15 - k) /2 - k + 1)** 2 ) * 2 * nb_channels)\n",
    "      \n",
    "            self.fc = nn.ModuleList([nn.Sequential(nn.Linear(out_conv , nb_hidden), nn.LeakyReLU(), nn.Dropout(p=0.2),\n",
    "                                                   nn.Linear(nb_hidden, 10) ) for i in range(self.nb_subnet) ] )\n",
    "                \n",
    "        #combine the subnetwork output\n",
    "        #(20,1) and sigmoid\n",
    "        self.comb = nn.Sequential(nn.Linear(20, 256), nn.LeakyReLU(), nn.Dropout(p=0.2),\n",
    "                                  nn.Linear(256, 256), nn.LeakyReLU(), nn.Dropout(p=0.2),\n",
    "                                  nn.Linear(256, 1), nn.Sigmoid() ) \n",
    "     \n",
    "            \n",
    "    def forward_once(self, x, subnet_nbr):\n",
    "        \n",
    "        x = self.conv[subnet_nbr](x)  #(batch_size,1,5,5)\n",
    "        \n",
    "        #need to flatten to insert in the fc layer\n",
    "        x = x.view(x.size(0), -1)    #(batch_size,25)\n",
    "        \n",
    "        x = self.fc[subnet_nbr](x)   #(batch_size,10)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, input1, input2): \n",
    "        \n",
    "        #we compute the output for each subnet \n",
    "        #apply conv and fc and get 10 units per subnetwork\n",
    "        output1 = self.forward_once(input1,0)\n",
    "        output2 = self.forward_once(input2, self.nb_subnet - 1)  #-1 because we access a list index starting at 0 \n",
    "        \n",
    "        #concat the two output \n",
    "        concat_subnet = torch.cat((output1, output2),1)     #get (batch_size,20)\n",
    "        \n",
    "        #forward on combination layer\n",
    "        output = self.comb(concat_subnet)\n",
    "        \n",
    "        return output1, output2, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ogskwx2nXWX"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSXNCrTNmjTu"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "def one_hot_encoding(input_, nb_classes): \n",
    "    tmp = input_.new_zeros(input_.size()[0], nb_classes)  #tensor of zeros (batch_size,10)\n",
    "    tmp.scatter_(1, input_, 1)     #fill with one in the correct index\n",
    "    return tmp\n",
    "\n",
    "def train_siamese(model, dataloader, test_data, epochs, learning_rate, aux_loss = False, alpha = 0.5 ):\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        model = model.to(device=\"cuda\")\n",
    "#         print(\"CUDA available\")\n",
    "#     else:\n",
    "#         print(\"NO CUDA\")\n",
    "        \n",
    "        \n",
    "    model.train() #set the model on training mode \n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    # gamma is the decaying factor, after every 1 epoch new_lr = lr*gamma \n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma = 0.9)\n",
    "    \n",
    "    if(aux_loss):\n",
    "        criterion_aux = nn.CrossEntropyLoss()\n",
    "        \n",
    "    print(\"Start training with {0} epochs, a learning rate of {1} and {2} as loss function\".format(epochs,learning_rate, criterion))\n",
    "        \n",
    "    if(aux_loss):\n",
    "        print(\"With auxilary loss function\")\n",
    "    else: \n",
    "        print(\"Without auxilary loss function\")\n",
    "            \n",
    "    if(model.nb_subnet == 1):\n",
    "        print(\"With weight sharing\")\n",
    "    else:\n",
    "        print(\"Without weight sharing\")\n",
    "           \n",
    "    training_losses = []\n",
    "    training_acc = []\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "    \n",
    "        sum_loss_epoch = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        accuracy_epoch = 0\n",
    "        \n",
    "        for ind_batch, sample_batched in enumerate(dataloader):\n",
    "    \n",
    "            images = sample_batched[\"images\"]                                  #(batch_size,2,14,14)\n",
    "            compare_labels = sample_batched[\"bool_labels\"].float().view(-1,1)  #(batch_size,1)\n",
    "            digit_labels = sample_batched[\"digit_labels\"]                      #(batch_size,2)\n",
    "            \n",
    "            if cuda:\n",
    "                images = images.to(device=\"cuda\")\n",
    "                compare_labels = compare_labels.to(device=\"cuda\")\n",
    "                digit_labels = digit_labels.to(device = \"cuda\")\n",
    "                \n",
    "            #gets (batch_size,1) and returns (batch_size,10)\n",
    "            one_hot_encoded_label1 = one_hot_encoding(digit_labels[:,0].view(-1,1), nb_classes=10)\n",
    "            one_hot_encoded_label2 = one_hot_encoding(digit_labels[:,1].view(-1,1), nb_classes=10)\n",
    "            \n",
    "            loss = 0\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            #seperate the 2 batches of images\n",
    "            input1 = images[:,0:1,:,:]   #(batch_size,1,14,14)\n",
    "            input2 = images[:,1:2,:,:]   #(batch_size,1,14,14)\n",
    "        \n",
    "            #compute the forward pass\n",
    "            output1, output2, output = model.forward(input1,input2)\n",
    "            \n",
    "            if(aux_loss):   \n",
    "                #Cross entropy loss but as already applying the soft_max activation function\n",
    "                #so only need to apply log\n",
    "                #add small value to avoid the log(0) problem\n",
    "                #multiplication element wise to keep only the probability of the correct label\n",
    "            \n",
    "                #loss_input1 = -torch.log(output1 + 1e-20) * one_hot_encoded_label1.float()   #(batch_size,1)\n",
    "                #loss_input2 = -torch.log(output2 + 1e-20) * one_hot_encoded_label2.float()   #(batch_size,1)\n",
    "                #loss = weight_loss_1 * loss_input1.mean() + weight_loss_2 * loss_input2.mean()  #auxilary loss\n",
    "                \n",
    "                loss1 =  criterion_aux(output1,digit_labels[:,0])\n",
    "                loss2 = criterion_aux(output2,digit_labels[:,1])\n",
    "                loss = (1 - alpha)/2 * loss1 + (1 - alpha)/2 * loss2 \n",
    "\n",
    "            loss3 = criterion(output, compare_labels)   #if batched do the mean of the errors \n",
    "            \n",
    "            loss += alpha * loss3\n",
    "\n",
    "            loss.require_grad = True   #should remove I think\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            #update the accuracy \n",
    "            total += images.size(0)  \n",
    "            correct += (output.round() == compare_labels).sum()  \n",
    "            \n",
    "            #if ind_batch % 1000 == 0:\n",
    "            #    print(\"[Epoch {}, Batch {}/{}]:  [Loss: {:.2f}]\".format(epoch, ind_batch, len(dataloader), loss) )\n",
    "                \n",
    "            #add the loss for this batch to the total loss of the epoch\n",
    "            sum_loss_epoch = sum_loss_epoch + loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        #compute the mean to obtain the loss for this epoch \n",
    "        mean_loss = sum_loss_epoch / float(len(dataloader))\n",
    "        \n",
    "        print('epoch {0}/{1}'.format(epoch, epochs))\n",
    "        \n",
    "#         print(\"At epoch {0} the loss is {1}\".format(epoch, mean_loss) )\n",
    "        training_losses.append(mean_loss)\n",
    "        \n",
    "        accuracy_epoch = float(correct) / float(total)\n",
    "#         print(\"At epoch {0} the accuracy is {1}\".format(epoch, accuracy_epoch) )\n",
    "        training_acc.append(accuracy_epoch)\n",
    "        \n",
    "        test_loss, test_accuracy = test_siamese(model, test_data, aux_loss, alpha)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "        test_acc.append(test_accuracy)\n",
    "        \n",
    "    return training_losses, training_acc, test_losses, test_acc\n",
    "\n",
    "def test_siamese(model, dataloader, aux_loss = False, alpha = 0.5):\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        model = model.to(device=\"cuda\")\n",
    "#         print(\"CUDA available\")\n",
    "#     else:\n",
    "#         print(\"NO CUDA\")\n",
    "        \n",
    "    model.eval() # set the model on evaluation mode\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    sum_test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    if(aux_loss):\n",
    "        criterion_aux = nn.CrossEntropyLoss()\n",
    "        \n",
    "    for ind_batch, sample_batched in enumerate(dataloader):\n",
    "        \n",
    "        images = sample_batched[\"images\"]                                  #(batch_size,2,14,14)\n",
    "        compare_labels = sample_batched[\"bool_labels\"].float().view(-1,1)  #(batch_size,1)\n",
    "        digit_labels = sample_batched[\"digit_labels\"]                      #(batch_size,2)    \n",
    "        \n",
    "        if cuda:\n",
    "                images = images.to(device=\"cuda\")\n",
    "                compare_labels = compare_labels.to(device=\"cuda\")\n",
    "                digit_labels = digit_labels.to(device = \"cuda\")\n",
    "                \n",
    "        #gets (batch_size,1) and returns (batch_size,10)\n",
    "        one_hot_encoded_label1 = one_hot_encoding(digit_labels[:,0].view(-1,1), nb_classes=10)\n",
    "        one_hot_encoded_label2 = one_hot_encoding(digit_labels[:,1].view(-1,1), nb_classes=10)        \n",
    "        \n",
    "        #seperate the 2 batches of images\n",
    "        input1 = images[:,0:1,:,:]   #(batch_size,1,14,14)\n",
    "        input2 = images[:,1:2,:,:]   #(batch_size,1,14,14)\n",
    "        \n",
    "        #compute the forward pass\n",
    "        output1, output2, output = model.forward(input1,input2)\n",
    "        \n",
    "        if(aux_loss):\n",
    "            #loss_input1 = -torch.log(output1 + 1e-20) * one_hot_encoded_label1.float()   #(batch_size,1)\n",
    "            #loss_input2 = -torch.log(output2 + 1e-20) * one_hot_encoded_label2.float()   #(batch_size,1)\n",
    "            #sum_test_loss += weight_loss_1 * loss_input1.mean() + weight_loss_2 * loss_input2.mean()  #auxilary loss\n",
    "            loss1 =  criterion_aux(output1,digit_labels[:,0])\n",
    "            loss2 = criterion_aux(output2,digit_labels[:,1])\n",
    "            sum_test_loss = (1 - alpha)/2 * loss1 + (1 - alpha)/2 * loss2  \n",
    "  \n",
    "        sum_test_loss += criterion(output, compare_labels) \n",
    "         \n",
    "        total += images.size(0)  \n",
    "        correct += (output.round() == compare_labels).sum() \n",
    "    \n",
    "    test_loss = sum_test_loss / float(len(dataloader))\n",
    "    test_acc = float(correct) / float(total)\n",
    "    \n",
    "    return test_loss.item(),test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xB3HoDanfDJ"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbOzyFPdmFUj"
   },
   "outputs": [],
   "source": [
    "######## DATA #####################################################\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, bool_labels, digit_labels = None):\n",
    "        self.images = data\n",
    "        self.bool_labels = bool_labels\n",
    "        \n",
    "        if digit_labels is not None:\n",
    "            self.digit_labels = digit_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # override the class method. return the length of data\n",
    "        return len(self.bool_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # override the class method. return the item at the index(idx)\n",
    "        if self.digit_labels is not None:\n",
    "            sample = {\"images\" : self.images[idx],\n",
    "                      \"bool_labels\" : self.bool_labels[idx],\n",
    "                      \"digit_labels\" : self.digit_labels[idx]}\n",
    "        else:\n",
    "            sample = {\"images\" : self.images[idx],\n",
    "                      \"bool_labels\" : self.bool_labels[idx]}\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "class SingleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, digit_labels):\n",
    "        self.images = data\n",
    "        self.digit_labels = digit_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # override the class method. return the length of data\n",
    "        return len(self.digit_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # override the class method. return the item at the index(idx)\n",
    "        sample = {\"images\" : self.images[idx],\n",
    "                  \"digit_labels\" : self.digit_labels[idx]}\n",
    "            \n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "pairs = generate_pair_sets(NB_SAMPLES)\n",
    "\n",
    "train_dataset = PairDataset(pairs[0], pairs[1], pairs[2])\n",
    "train_dataloader = data.DataLoader(dataset=train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = PairDataset(pairs[3], pairs[4], pairs[5])\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "gridsearch_params = [\n",
    "    (kernel_size, nb_channels, fc, alpha)\n",
    "   \n",
    "    for kernel_size in [3,5]\n",
    "    for nb_channels in [2,4,8,16,24,48]\n",
    "    for fc in [32,64,128, 256]\n",
    "    for alpha in np.linspace(0, 1, 5)\n",
    "]\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PC3knXpHnjo_"
   },
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jdx0I5gEthZj"
   },
   "outputs": [],
   "source": [
    "from tempfile import TemporaryFile\n",
    "outfile = TemporaryFile()\n",
    "a=np.array([1,2])\n",
    "np.savez(\"a\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ivp1elLgiYdJ",
    "outputId": "91425544-4245-4b45-dd76-31d24b1c279d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO CUDA\n",
      "round 0 start\n",
      "Start training with 20 epochs, a learning rate of 0.001 and BCELoss() as loss function\n",
      "With auxilary loss function\n",
      "With weight sharing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0fc39261b7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                                              \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                                              \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                                                              aux_loss = True)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'With parameters {0} kernel size, {1} nb_channels, {2} nb_hidden fc,  {3} alpha: '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e1f95e1fe055>\u001b[0m in \u001b[0;36mtrain_siamese\u001b[0;34m(model, dataloader, test_data, epochs, learning_rate, aux_loss, alpha)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m#compute the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-51e5df61670d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input1, input2)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m#we compute the output for each subnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m#apply conv and fc and get 10 units per subnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_subnet\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#-1 because we access a list index starting at 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-51e5df61670d>\u001b[0m in \u001b[0;36mforward_once\u001b[0;34m(self, x, subnet_nbr)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubnet_nbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubnet_nbr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#(batch_size,1,5,5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m#need to flatten to insert in the fc layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/Python/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/Python/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/Python/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/Python/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/Python/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"CUDA available\")\n",
    "else:\n",
    "    print(\"NO CUDA\")\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 20\n",
    "rounds = 5\n",
    "round_results = [] #3D\n",
    "\n",
    "for i in range(rounds):\n",
    "    results = [] #training_losses, training_acc, test_losses, test_acc\n",
    "    \n",
    "    print('round {0} start'.format(i))\n",
    "    \n",
    "    for kernel_size, c, n, a in gridsearch_params:\n",
    "        \n",
    "        model = siamese_net(weight_sharing = True , architecture = 5, \n",
    "                            nb_channels = c, nb_hidden = n, k = kernel_size )\n",
    "        training_losses, training_acc, test_losses, test_acc = train_siamese(model,\n",
    "                                                                             train_dataloader, \n",
    "                                                                             test_dataloader, \n",
    "                                                                             epochs,\n",
    "                                                                             lr, \n",
    "                                                                             alpha = a,\n",
    "                                                                             aux_loss = True)\n",
    "        \n",
    "        print('With parameters {0} kernel size, {1} nb_channels, {2} nb_hidden fc,  {3} alpha: '.format(kernel_size, c, n, a))\n",
    "        final_test_loss, final_test_loss_acc = test_siamese(model, test_dataloader, aux_loss = True)\n",
    "        print(\"On the test set we obtain a loss of {:.2f} and an accuracy of {:.2f}\".format(final_test_loss,final_test_loss_acc))\n",
    "        \n",
    "        results.append([training_losses, training_acc, test_losses, test_acc])\n",
    "    \n",
    "    \n",
    "    print('round {0} end'.format(i))\n",
    "    round_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MbH2yMO_skRZ"
   },
   "outputs": [],
   "source": [
    "np.savez(\"results\",round_results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

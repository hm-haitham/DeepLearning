{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING PROJECT 1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/drive/My Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "NB_SAMPLES = 1000\n",
    "DATA_DIR = './data'\n",
    "\n",
    "NUMBER_OF_CLASSES = 10\n",
    "\n",
    "WIDTH_HEIGHT = 14\n",
    "SINGLE_IMAGE_SIZE = WIDTH_HEIGHT * WIDTH_HEIGHT\n",
    "DOUBLE_IMAGE_SIZE = 2 * SINGLE_IMAGE_SIZE\n",
    "\n",
    "# ----Train Config-----#\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_BATCH_SIZE = 5\n",
    "SUB_CRITERION = nn.CrossEntropyLoss()\n",
    "FINAL_CRITERION = nn.BCELoss()\n",
    "EPOCHS = 20\n",
    "\n",
    "# ----AuxLoss Config-----#\n",
    "ALPHA = 0.5\n",
    "\n",
    "# ----Search Config-----#\n",
    "KERNEL_SIZES = [3,5]\n",
    "NB_CHANNELS = [4,8,16,24,48]\n",
    "FCNEURONS = [32,64,128, 256,512]\n",
    "NB_LAYERS = [1, 2, 3]\n",
    "ALPHAS = np.linspace(0, 1, 10)\n",
    "\n",
    "#----Test Config-----#\n",
    "TEST_BATCH_SIZE = NB_SAMPLES\n",
    "\n",
    "#----BasicNet Config-----#\n",
    "BASIC_NET_NAME = \"basic_net\"\n",
    "BASIC_NET_HIDDEN_LAYER = 128\n",
    "BASIC_NET_NB_HIDDEN = 1\n",
    "\n",
    "#----OscarNet Config-----#\n",
    "OSCAR_NET_NAME = \"oscar_net\"\n",
    "OSCAR_NET_HIDDEN_LAYER = 128\n",
    "OSCAR_NET_NB_HIDDEN = 1\n",
    "\n",
    "#----RobertNet Config-----#\n",
    "ROBERT_NET_NAME = \"robert_net\"\n",
    "ROBERT_NET_HIDDEN_LAYER = 128\n",
    "ROBERT_NET_BASE_CHANNEL_SIZE = 8\n",
    "ROBERT_NET_NB_HIDDEN = 1\n",
    "\n",
    "#----MaryJaneNet Config-----#\n",
    "MARYJANE_NET_NAME = \"maryjane_net\"\n",
    "MARYJANE_NET_HIDDEN_LAYER = 128\n",
    "MARYJANE_NET_BASE_CHANNEL_SIZE = 8\n",
    "MARYJANE_NET_NB_HIDDEN = 1\n",
    "MARYJANE_NET_KERNEL_SIZE = 3\n",
    "\n",
    "#----DesmondNet Config-----#\n",
    "DESMOND_NET_NAME = \"desmond_net\"\n",
    "DESMOND_NET_HIDDEN_LAYER = 256\n",
    "DESMOND_NET_NB_HIDDEN = 2\n",
    "\n",
    "#----LeonardtNet Config-----#\n",
    "LEONARD_NET_NAME = \"leonard_net\"\n",
    "LEONARD_NET_HIDDEN_LAYER = 256\n",
    "LEONARD_NET_NB_HIDDEN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# HELPERS ###############################\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# The data\n",
    "\n",
    "def convert_to_one_hot_labels(input, target):\n",
    "    tmp = input.new_zeros(target.size(0), target.max() + 1)\n",
    "    #set ones\n",
    "    tmp.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return tmp\n",
    "\n",
    "def load_data(cifar = None, one_hot_labels = False, normalize = False, flatten = True):\n",
    "\n",
    "    data_dir = './data'\n",
    "\n",
    "    if (cifar is not None and cifar):\n",
    "        print('* Using CIFAR')\n",
    "        cifar_train_set = datasets.CIFAR10(data_dir + '/cifar10/', train = True, download = True)\n",
    "        cifar_test_set = datasets.CIFAR10(data_dir + '/cifar10/', train = False, download = True)\n",
    "\n",
    "        train_input = torch.from_numpy(cifar_train_set.data)\n",
    "        train_input = train_input.transpose(3, 1).transpose(2, 3).float()\n",
    "        train_target = torch.tensor(cifar_train_set.targets, dtype = torch.int64)\n",
    "\n",
    "        test_input = torch.from_numpy(cifar_test_set.data).float()\n",
    "        test_input = test_input.transpose(3, 1).transpose(2, 3).float()\n",
    "        test_target = torch.tensor(cifar_test_set.targets, dtype = torch.int64)\n",
    "\n",
    "    else:\n",
    "        print('* Using MNIST')\n",
    "        mnist_train_set = datasets.MNIST(data_dir + '/mnist/', train = True, download = True)\n",
    "        mnist_test_set = datasets.MNIST(data_dir + '/mnist/', train = False, download = True)\n",
    "\n",
    "        train_input = mnist_train_set.data.view(-1, 1, 28, 28).float()\n",
    "        train_target = mnist_train_set.targets\n",
    "        test_input = mnist_test_set.data.view(-1, 1, 28, 28).float()\n",
    "        test_target = mnist_test_set.targets\n",
    "\n",
    "    if flatten:\n",
    "        train_input = train_input.clone().reshape(train_input.size(0), -1)\n",
    "        test_input = test_input.clone().reshape(test_input.size(0), -1)\n",
    "        \n",
    "        \n",
    "    train_input = train_input.narrow(0, 0, 1000)\n",
    "    train_target = train_target.narrow(0, 0, 1000)\n",
    "    test_input = test_input.narrow(0, 0, 1000)\n",
    "    test_target = test_target.narrow(0, 0, 1000)\n",
    "\n",
    "    print('** Use {:d} train and {:d} test samples'.format(train_input.size(0), test_input.size(0)))\n",
    "\n",
    "    if one_hot_labels:\n",
    "        train_target = convert_to_one_hot_labels(train_input, train_target)\n",
    "        test_target = convert_to_one_hot_labels(test_input, test_target)\n",
    "\n",
    "    if normalize:\n",
    "        mu, std = train_input.mean(), train_input.std()\n",
    "        train_input.sub_(mu).div_(std)\n",
    "        test_input.sub_(mu).div_(std)\n",
    "\n",
    "    return train_input, train_target, test_input, test_target\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def mnist_to_pairs(nb, input, target):\n",
    "    input = torch.functional.F.avg_pool2d(input, kernel_size = 2)\n",
    "    a = torch.randperm(input.size(0))\n",
    "    a = a[:2 * nb].view(nb, 2)\n",
    "    input = torch.cat((input[a[:, 0]], input[a[:, 1]]), 1)\n",
    "    classes = target[a]\n",
    "    target = (classes[:, 0] <= classes[:, 1]).long()\n",
    "    return input, target, classes\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def generate_pair_sets(nb):\n",
    "\n",
    "    data_dir = DATA_DIR\n",
    "\n",
    "    train_set = datasets.MNIST(data_dir + '/mnist/', train = True, download = True)\n",
    "    train_input = train_set.data.view(-1, 1, 28, 28).float()\n",
    "    train_target = train_set.targets\n",
    "\n",
    "    test_set = datasets.MNIST(data_dir + '/mnist/', train = False, download = True)\n",
    "    test_input = test_set.data.view(-1, 1, 28, 28).float()\n",
    "    test_target = test_set.targets\n",
    "\n",
    "    return mnist_to_pairs(nb, train_input, train_target) + \\\n",
    "           mnist_to_pairs(nb, test_input, test_target)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def compute_accuracy(tensor1, tensor2):\n",
    "    \n",
    "    tensor_accuracy = torch.where(tensor1 == tensor2, torch.tensor(1), torch.tensor(0))\n",
    "    \n",
    "    accuracy = torch.sum(tensor_accuracy).item() / NB_SAMPLES\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def save_model(model, epoch=None, loss=None, save_dir=None, specific_name=None):\n",
    "\n",
    "    if epoch and loss and save_dir and specific_name:\n",
    "        model_name = model.model_name\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        file_name = f\"{timestr}_{model_name}_epoch_{epoch}_loss_{loss:03.3f}.pt\"\n",
    "        Path(save_dir).mkdir(exist_ok=True)\n",
    "        file_path = Path(save_dir) / file_name\n",
    "        torch.save(model.state_dict(), str(file_path))\n",
    "    elif save_dir and specific_name:\n",
    "        file_path = Path(save_dir) / specific_name\n",
    "        torch.save(model.state_dict(), str(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "class MaryJaneNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_hidden_layers = MARYJANE_NET_NB_HIDDEN, base_channel_size = MARYJANE_NET_BASE_CHANNEL_SIZE, hidden_layer = MARYJANE_NET_HIDDEN_LAYER, kernel_size = MARYJANE_NET_KERNEL_SIZE):\n",
    "        \n",
    "        super(MaryJaneNet, self).__init__()\n",
    "        self.model_name = MARYJANE_NET_NAME\n",
    "        \n",
    "        if nb_hidden_layers < 0:\n",
    "             raise Exception(\"Minimum 0 hidden layers for \" + self.model_name)\n",
    "        \n",
    "        self.base_channel_size = base_channel_size\n",
    "        \n",
    "        conv_channel_size = self.base_channel_size*2\n",
    "        \n",
    "        #Change to Module list instead of Sequential if the number of ConvNets is dynamic (i.e passed as parameter)\n",
    "        #(Wâˆ’F+2P)/S+1\n",
    "        #k= 1, 3, 5  \n",
    "        self.conv_net = nn.Sequential(nn.Conv2d(1, self.base_channel_size, kernel_size= kernel_size ),  #(14-k)+1 = 15 -k # 14, 12,10\n",
    "                                                 nn.LeakyReLU(),\n",
    "                                                 nn.MaxPool2d(kernel_size=2, stride=2), #(15-k) / 2 : 7, 6, 5  \n",
    "                                                 nn.Dropout(p=0.2),\n",
    "                                                 nn.Conv2d(self.base_channel_size, conv_channel_size, \n",
    "                                                           kernel_size = kernel_size),    #7, 5, 3   # (15 - k) /2 - k + 1\n",
    "                                                 nn.LeakyReLU(),\n",
    "                                                 #nn.MaxPool2d(kernel_size=3, stride=3), #1  \n",
    "                                                 nn.Dropout(p=0.2) )\n",
    "\n",
    "        fcn_input_size = int( ( ((15 - kernel_size) /2 - kernel_size + 1)** 2 ) * conv_channel_size)\n",
    "        \n",
    "        self.fc_net = nn.ModuleList()\n",
    "        \n",
    "        if nb_hidden_layers > 0:\n",
    "            self.fc_net = nn.ModuleList([nn.Sequential(nn.Linear(hidden_layer, hidden_layer), nn.LeakyReLU(), nn.Dropout(p=0.2)) for i in range(nb_hidden_layers-1)])\n",
    "\n",
    "            self.fc_net.insert(0,nn.Sequential(nn.Linear(fcn_input_size, hidden_layer), nn.LeakyReLU(), nn.Dropout(p=0.2)))\n",
    "\n",
    "            self.output = nn.Linear(hidden_layer, NUMBER_OF_CLASSES)\n",
    "            \n",
    "        if nb_hidden_layers == 0:\n",
    "            self.output = nn.Linear(fcn_input_size, NUMBER_OF_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conved = self.conv_net(x)\n",
    "        \n",
    "        flattened = conved.view(conved.size(0),-1)\n",
    "        \n",
    "        hid = flattened\n",
    "        \n",
    "        for block in self.fc_net:\n",
    "            hid = block(hid)\n",
    "        \n",
    "        out = self.output(hid)\n",
    "        \n",
    "        return F.softmax(out, dim=1), out\n",
    "\n",
    "\n",
    "class LeonardNet(nn.Module):\n",
    "\n",
    "    def __init__(self, image_net, nb_hidden_layers= LEONARD_NET_NB_HIDDEN, hidden_layer=LEONARD_NET_HIDDEN_LAYER):\n",
    "        super(LeonardNet, self).__init__()\n",
    "        self.model_name = LEONARD_NET_NAME\n",
    "        \n",
    "        if nb_hidden_layers < 1 :\n",
    "            raise Exception(\"Minimum 1 hidden layer\")\n",
    "        \n",
    "        self.image_net = image_net\n",
    "        \n",
    "        self.hiddens = nn.ModuleList()\n",
    "        \n",
    "        if nb_hidden_layers > 0:\n",
    "            self.hiddens = nn.ModuleList([nn.Sequential(nn.Linear(hidden_layer, hidden_layer), nn.LeakyReLU(), nn.Dropout(p=0.2)) for i in range(nb_hidden_layers-1)])\n",
    "\n",
    "            self.hiddens.insert(0,nn.Sequential(nn.Linear(NUMBER_OF_CLASSES*2, hidden_layer), nn.LeakyReLU(), nn.Dropout(p=0.2)))\n",
    "\n",
    "            self.output = nn.Linear(hidden_layer, 1)\n",
    "            \n",
    "        if nb_hidden_layers == 0:\n",
    "            self.output = nn.Linear(NUMBER_OF_CLASSES*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #SPLIT x which is of size [N, 2, 14, 14] to two distinct tensors of size [N, 1, 14, 14]\n",
    "        input1 = x[:,0:1,:,:]   #(batch_size,1,14,14)\n",
    "        input2 = x[:,1:2,:,:]   #(batch_size,1,14,14)\n",
    "        \n",
    "        lefted, lefted_no = self.image_net(input1)\n",
    "        \n",
    "        righted, righted_no = self.image_net(input2)\n",
    "        \n",
    "        #CONCAT lefted and righted which are of size [N,10] each to a single tensor of size [N,20]\n",
    "        hid = torch.cat((lefted, righted),1)\n",
    "        \n",
    "        for block in self.hiddens:\n",
    "            hid = block(hid)\n",
    "        \n",
    "        out = self.output(hid)\n",
    "        \n",
    "        return torch.sigmoid(out), lefted_no, righted_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_siamese(model, \n",
    "                  dataloader, \n",
    "                  test_dataloader,\n",
    "                  epochs = EPOCHS,\n",
    "                  final_criterion = FINAL_CRITERION, \n",
    "                  learning_rate = LEARNING_RATE,\n",
    "                  aux_loss = False,\n",
    "                  sub_criterion = SUB_CRITERION, \n",
    "                  alpha = ALPHA):\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        model = model.to(device=\"cuda\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # gamma is the decaying factor, after every 1 epoch new_lr = lr*gamma \n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma = 0.9)\n",
    "\n",
    "    training_losses = []\n",
    "    training_acc = []\n",
    "    \n",
    "    training_losses_l = []\n",
    "    \n",
    "    training_losses_r = []\n",
    "    \n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    \n",
    "    test_losses_l = []\n",
    "    \n",
    "    test_losses_r = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):  \n",
    "        model.train()\n",
    "        \n",
    "        sum_loss_epoch = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        accuracy_epoch = 0\n",
    "        \n",
    "        sum_loss_epoch_l = 0\n",
    "        \n",
    "        sum_loss_epoch_r = 0\n",
    "        \n",
    "        for ind_batch, sample_batched in enumerate(dataloader):\n",
    "            \n",
    "            images = sample_batched[\"images\"]\n",
    "            labels = sample_batched[\"bool_labels\"]\n",
    "            digit_labels = sample_batched[\"digit_labels\"]\n",
    "            \n",
    "            labels = labels.unsqueeze(1)\n",
    "            \n",
    "            if cuda:\n",
    "                images = images.to(device=\"cuda\")\n",
    "                labels = labels.to(device=\"cuda\")\n",
    "                digit_labels = digit_labels.to(device=\"cuda\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "                       \n",
    "            output, lefted, righted = model(images)\n",
    "            \n",
    "            loss = final_criterion(output.flatten(), labels.float().flatten())\n",
    "            loss_left = sub_criterion(lefted, digit_labels[:,0])\n",
    "            loss_right = sub_criterion(righted, digit_labels[:,1])\n",
    "            \n",
    "            if aux_loss:\n",
    "                loss = alpha * loss + ((1-alpha)/2) * loss_left + ((1-alpha)/2) * loss_right\n",
    "\n",
    "            loss.require_grad = True\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            #update the accuracy \n",
    "            total += images.size(0)  \n",
    "            correct += (output.round() == labels).sum() \n",
    "            \n",
    "            # if ind_batch % 250 == 0:\n",
    "                # print(\"[Epoch {}, Batch {}/{}]:  [Loss: {:.2f}]\".format(epoch, ind_batch, len(dataloader), loss) )\n",
    "                \n",
    "            #add the loss for this batch to the total loss of the epoch\n",
    "            sum_loss_epoch = sum_loss_epoch + loss.item()\n",
    "            sum_loss_epoch_l = sum_loss_epoch_l + loss_left.item()\n",
    "            sum_loss_epoch_r = sum_loss_epoch_r + loss_right.item()\n",
    "            \n",
    "        scheduler.step()\n",
    "        #compute the mean to obtain the loss for this epoch \n",
    "        mean_loss = sum_loss_epoch / float(len(dataloader))\n",
    "        mean_loss_l = sum_loss_epoch_l / float(len(dataloader))\n",
    "        mean_loss_r = sum_loss_epoch_r / float(len(dataloader))\n",
    "        \n",
    "        # print(\"At epoch {0} the training loss is {1}\".format(epoch, mean_loss) )\n",
    "        training_losses.append(mean_loss)\n",
    "        \n",
    "        accuracy_epoch = float(correct) / float(total)\n",
    "        # print(\"At epoch {0} the training accuracy is {1}\".format(epoch, accuracy_epoch) )\n",
    "        training_acc.append(accuracy_epoch)\n",
    "        \n",
    "        training_losses_l.append(mean_loss_l)\n",
    "        training_losses_r.append(mean_loss_r)\n",
    "        \n",
    "        print('epoch {0}/{1}'.format(epoch, epochs))\n",
    "        \n",
    "        test_loss, test_accuracy, test_loss_l, test_loss_r = predict_siamese(model,\n",
    "                                                                     test_dataloader,\n",
    "                                                                     final_criterion,\n",
    "                                                                     aux_loss,\n",
    "                                                                     sub_criterion,\n",
    "                                                                     alpha)\n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "        test_acc.append(test_accuracy)\n",
    "        test_losses_l.append(test_loss_l)\n",
    "        test_losses_r.append(test_loss_r)\n",
    "        \n",
    "    return training_losses, training_acc, training_losses_l, training_losses_r, test_losses, test_acc, test_losses_l, test_losses_r\n",
    "\n",
    "\n",
    "def predict_siamese(model, \n",
    "            dataloader,\n",
    "            final_criterion = FINAL_CRITERION,\n",
    "            aux_loss = False,\n",
    "            sub_criterion = SUB_CRITERION, \n",
    "            alpha = ALPHA):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        model = model.to(device=\"cuda\")\n",
    "        \n",
    "    sum_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    sum_loss_l = 0\n",
    "\n",
    "    sum_loss_r = 0\n",
    "\n",
    "    for ind_batch, sample_batched in enumerate(dataloader):\n",
    "\n",
    "        images = sample_batched[\"images\"]\n",
    "        labels = sample_batched[\"bool_labels\"]\n",
    "        digit_labels = sample_batched[\"digit_labels\"]\n",
    "        \n",
    "        if cuda:\n",
    "            images = images.to(device=\"cuda\")\n",
    "            labels = labels.to(device=\"cuda\")\n",
    "            digit_labels = digit_labels.to(device=\"cuda\")\n",
    "\n",
    "        output, lefted, righted = model(images)\n",
    "        \n",
    "        labels = labels.unsqueeze(1)\n",
    "\n",
    "        loss = final_criterion(output.flatten(), labels.float().flatten())\n",
    "        loss_left = sub_criterion(lefted, digit_labels[:,0])\n",
    "        loss_right = sub_criterion(righted, digit_labels[:,1])\n",
    "\n",
    "        if aux_loss:\n",
    "            loss = alpha * loss + ((1-alpha)/2) * loss_left + ((1-alpha)/2) * loss_right\n",
    "\n",
    "        #update the accuracy \n",
    "        total += images.size(0)  \n",
    "        correct += (output.round() == labels).sum() \n",
    "\n",
    "        #add the loss for this batch to the total loss of the epoch\n",
    "        sum_loss = sum_loss + loss.item()\n",
    "        sum_loss_l = sum_loss_l + loss_left.item()\n",
    "        sum_loss_r = sum_loss_r + loss_right.item()\n",
    "\n",
    "    #compute the mean to obtain the loss for this epoch \n",
    "    mean_loss = sum_loss / float(len(dataloader))\n",
    "    mean_loss_l = sum_loss_l / float(len(dataloader))\n",
    "    mean_loss_r = sum_loss_r / float(len(dataloader))\n",
    "    \n",
    "    # print(\"The test loss is {0}\".format(mean_loss) )\n",
    "\n",
    "    accuracy = float(correct) / float(total)\n",
    "    # print(\"The test accuracy is {0}\".format(accuracy) )\n",
    "        \n",
    "    return mean_loss, accuracy, mean_loss_l, mean_loss_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## DATA #####################################################\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, bool_labels, digit_labels = None):\n",
    "        self.images = data\n",
    "        self.bool_labels = bool_labels\n",
    "        \n",
    "        if digit_labels is not None:\n",
    "            self.digit_labels = digit_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # override the class method. return the length of data\n",
    "        return len(self.bool_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # override the class method. return the item at the index(idx)\n",
    "        if self.digit_labels is not None:\n",
    "            sample = {\"images\" : self.images[idx],\n",
    "                      \"bool_labels\" : self.bool_labels[idx],\n",
    "                      \"digit_labels\" : self.digit_labels[idx]}\n",
    "        else:\n",
    "            sample = {\"images\" : self.images[idx],\n",
    "                      \"bool_labels\" : self.bool_labels[idx]}\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "class SingleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, digit_labels):\n",
    "        self.images = data\n",
    "        self.digit_labels = digit_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # override the class method. return the length of data\n",
    "        return len(self.digit_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # override the class method. return the item at the index(idx)\n",
    "        sample = {\"images\" : self.images[idx],\n",
    "                  \"digit_labels\" : self.digit_labels[idx]}\n",
    "            \n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "pairs = generate_pair_sets(NB_SAMPLES)\n",
    "\n",
    "train_dataset = PairDataset(pairs[0], pairs[1], pairs[2])\n",
    "train_dataloader = data.DataLoader(dataset=train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = PairDataset(pairs[3], pairs[4], pairs[5])\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE_CHANNEL_SIZE search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"CUDA available\")\n",
    "else:\n",
    "    print(\"NO CUDA\")\n",
    "\n",
    "rounds = 10\n",
    "round_results_nb_channels = [] #3D\n",
    "\n",
    "for i in range(rounds):\n",
    "    results = [] #training_losses, training_acc, test_losses, test_acc\n",
    "    \n",
    "    print('round {0} start'.format(i+1))\n",
    "    \n",
    "    for ind, c in enumerate(NB_CHANNELS):\n",
    "        \n",
    "        classifier = MaryJaneNet(base_channel_size = c)\n",
    "\n",
    "        model = LeonardNet(image_net = classifier)\n",
    "        \n",
    "        training_losses, training_acc, _, _, test_losses, test_acc, _, _ = train_siamese(model = model,\n",
    "                                     dataloader = train_dataloader,\n",
    "                                     test_dataloader = test_dataloader,\n",
    "                                     epochs = EPOCHS,\n",
    "                                     final_criterion = FINAL_CRITERION, \n",
    "                                     learning_rate = LEARNING_RATE,\n",
    "                                     aux_loss = True,\n",
    "                                     sub_criterion = SUB_CRITERION, \n",
    "                                     alpha = ALPHA)\n",
    "        \n",
    "        print('{0}/{1}'.format(ind+1, len(NB_CHANNELS)))\n",
    "        print('With parameters kernel_size/base_channel/nb_hid_neur_inner/nb_hid_neur_out/alpha : {0}/{1}/{2}/{3}/{4}'.format(MARYJANE_NET_KERNEL_SIZE,\n",
    "                                                                                                                              c, \n",
    "                                                                                                                              MARYJANE_NET_HIDDEN_LAYER,\n",
    "                                                                                                                              LEONARD_NET_HIDDEN_LAYER,\n",
    "                                                                                                                              ALPHA))\n",
    "        final_test_loss, final_test_loss_acc = test_losses[-1], test_acc[-1]\n",
    "        print(\"On the test set we obtain a loss of {:.2f} and an accuracy of {:.2f}\".format(final_test_loss,final_test_loss_acc))\n",
    "        \n",
    "        results.append([training_losses, training_acc, test_losses, test_acc])\n",
    "    \n",
    "    \n",
    "    print('round {0} end'.format(i+1))\n",
    "    round_results_nb_channels.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"results-batch{0}-nb_channels_search\".format(TRAIN_BATCH_SIZE),round_results_nb_channels)\n",
    "\n",
    "copy_of = round_results_nb_channels.copy()\n",
    "\n",
    "print('Shape of \"copy_of\" : {0}'.format(copy_of.shape))\n",
    "\n",
    "last_accs_only = copy_of[:, :, 3, 19]\n",
    "\n",
    "print('Shape of \"last_accs_only\" : {0}'.format(last_accs_only.shape))\n",
    "\n",
    "means_ch = last_accs_only.mean(axis=0)\n",
    "stds_ch = last_accs_only.std(axis=0)\n",
    "\n",
    "print(means_ch)\n",
    "print(stds_ch)\n",
    "\n",
    "i_max = np.argmax(means_ch)\n",
    "\n",
    "print(i_max)\n",
    "\n",
    "best_ch_size = NB_CHANNELS[i_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCNEURONS (inner network) search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"CUDA available\")\n",
    "else:\n",
    "    print(\"NO CUDA\")\n",
    "\n",
    "rounds = 10\n",
    "round_results_nb_neurons_inner = [] #3D\n",
    "\n",
    "for i in range(rounds):\n",
    "    results = [] #training_losses, training_acc, test_losses, test_acc\n",
    "    \n",
    "    print('round {0} start'.format(i+1))\n",
    "    \n",
    "    for ind, n in enumerate(FCNEURONS):\n",
    "        \n",
    "        classifier = MaryJaneNet(base_channel_size = best_ch_size, hidden_layer = n)\n",
    "\n",
    "        model = LeonardNet(image_net = classifier)\n",
    "        \n",
    "        training_losses, training_acc, _, _, test_losses, test_acc, _, _ = train_siamese(model = model,\n",
    "                                     dataloader = train_dataloader,\n",
    "                                     test_dataloader = test_dataloader,\n",
    "                                     epochs = EPOCHS,\n",
    "                                     final_criterion = FINAL_CRITERION, \n",
    "                                     learning_rate = LEARNING_RATE,\n",
    "                                     aux_loss = True,\n",
    "                                     sub_criterion = SUB_CRITERION, \n",
    "                                     alpha = ALPHA)\n",
    "        \n",
    "        print('{0}/{1}'.format(ind+1, len(NB_CHANNELS)))\n",
    "        print('With parameters kernel_size/base_channel/nb_hid_neur_inner/nb_hid_neur_out/alpha : {0}/{1}/{2}/{3}/{4}'.format(MARYJANE_NET_KERNEL_SIZE,\n",
    "                                                                                                                              best_ch_size, \n",
    "                                                                                                                              n,\n",
    "                                                                                                                              LEONARD_NET_HIDDEN_LAYER,\n",
    "                                                                                                                              ALPHA))\n",
    "        final_test_loss, final_test_loss_acc = test_losses[-1], test_acc[-1]\n",
    "        print(\"On the test set we obtain a loss of {:.2f} and an accuracy of {:.2f}\".format(final_test_loss,final_test_loss_acc))\n",
    "        \n",
    "        results.append([training_losses, training_acc, test_losses, test_acc])\n",
    "    \n",
    "    \n",
    "    print('round {0} end'.format(i+1))\n",
    "    round_results_nb_neurons_inner.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4, 20)\n",
      "(10, 5)\n",
      "(5,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "np.savez(\"results-batch{0}-nb_fcneurons_inner_search\".format(TRAIN_BATCH_SIZE),round_results_nb_neurons_inner)\n",
    "\n",
    "copy_of = round_results_nb_neurons_inner.copy()\n",
    "\n",
    "print('Shape of \"copy_of\" : {0}'.format(copy_of.shape))\n",
    "\n",
    "last_accs_only = copy_of[:, :, 3, 19]\n",
    "\n",
    "print('Shape of \"last_accs_only\" : {0}'.format(last_accs_only.shape))\n",
    "\n",
    "means_fcni = last_accs_only.mean(axis=0)\n",
    "stds_fcni = last_accs_only.std(axis=0)\n",
    "\n",
    "print(means_fcni)\n",
    "print(stds_fcni)\n",
    "\n",
    "i_max = np.argmax(means_fcni)\n",
    "\n",
    "print(i_max)\n",
    "\n",
    "best_fcni = FCNEURONS[i_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERNEL_SIZE (inner network) search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"CUDA available\")\n",
    "else:\n",
    "    print(\"NO CUDA\")\n",
    "\n",
    "rounds = 10\n",
    "round_results_kernel = [] #3D\n",
    "\n",
    "for i in range(rounds):\n",
    "    results = [] #training_losses, training_acc, test_losses, test_acc\n",
    "    \n",
    "    print('round {0} start'.format(i+1))\n",
    "    \n",
    "    for ind, k in enumerate(KERNEL_SIZES):\n",
    "        \n",
    "        classifier = MaryJaneNet(base_channel_size = best_ch_size, hidden_layer = best_fcni, kernel_size=k)\n",
    "\n",
    "        model = LeonardNet(image_net = classifier)\n",
    "        \n",
    "        training_losses, training_acc, _, _, test_losses, test_acc, _, _ = train_siamese(model = model,\n",
    "                                     dataloader = train_dataloader,\n",
    "                                     test_dataloader = test_dataloader,\n",
    "                                     epochs = EPOCHS,\n",
    "                                     final_criterion = FINAL_CRITERION, \n",
    "                                     learning_rate = LEARNING_RATE,\n",
    "                                     aux_loss = True,\n",
    "                                     sub_criterion = SUB_CRITERION, \n",
    "                                     alpha = ALPHA)\n",
    "        \n",
    "        print('{0}/{1}'.format(ind+1, len(NB_CHANNELS)))\n",
    "        print('With parameters kernel_size/base_channel/nb_hid_neur_inner/nb_hid_neur_out/alpha : {0}/{1}/{2}/{3}/{4}'.format(k,\n",
    "                                                                                                                              best_ch_size, \n",
    "                                                                                                                              best_fcni,\n",
    "                                                                                                                              LEONARD_NET_HIDDEN_LAYER,\n",
    "                                                                                                                              ALPHA))\n",
    "        final_test_loss, final_test_loss_acc = test_losses[-1], test_acc[-1]\n",
    "        print(\"On the test set we obtain a loss of {:.2f} and an accuracy of {:.2f}\".format(final_test_loss,final_test_loss_acc))\n",
    "        \n",
    "        results.append([training_losses, training_acc, test_losses, test_acc])\n",
    "    \n",
    "    \n",
    "    print('round {0} end'.format(i+1))\n",
    "    round_results_kernel.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"results-batch{0}-nb_kernel_search\".format(TRAIN_BATCH_SIZE),round_results_kernel)\n",
    "\n",
    "copy_of = round_results_kernel.copy()\n",
    "\n",
    "print('Shape of \"copy_of\" : {0}'.format(copy_of.shape))\n",
    "\n",
    "last_accs_only = copy_of[:, :, 3, 19]\n",
    "\n",
    "print('Shape of \"last_accs_only\" : {0}'.format(last_accs_only.shape))\n",
    "\n",
    "means_kernel = last_accs_only.mean(axis=0)\n",
    "stds_kernel = last_accs_only.std(axis=0)\n",
    "\n",
    "print(means_fcni)\n",
    "print(stds_fcni)\n",
    "\n",
    "i_max = np.argmax(means_kernel)\n",
    "\n",
    "print(i_max)\n",
    "\n",
    "best_kernel = KERNEL_SIZES[i_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCNEURONS (outer network) search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"CUDA available\")\n",
    "else:\n",
    "    print(\"NO CUDA\")\n",
    "\n",
    "rounds = 10\n",
    "round_results_nb_neurons_outer = [] #3D\n",
    "\n",
    "for i in range(rounds):\n",
    "    results = [] #training_losses, training_acc, test_losses, test_acc\n",
    "    \n",
    "    print('round {0} start'.format(i+1))\n",
    "    \n",
    "    for ind, n in enumerate(FCNEURONS):\n",
    "        \n",
    "        classifier = MaryJaneNet(base_channel_size = best_ch_size, hidden_layer = best_fcni, kernel_size = best_kernel)\n",
    "\n",
    "        model = LeonardNet(image_net = classifier, hidden_layer = n)\n",
    "        \n",
    "        training_losses, training_acc, _, _, test_losses, test_acc, _, _ = train_siamese(model = model,\n",
    "                                     dataloader = train_dataloader,\n",
    "                                     test_dataloader = test_dataloader,\n",
    "                                     epochs = EPOCHS,\n",
    "                                     final_criterion = FINAL_CRITERION, \n",
    "                                     learning_rate = LEARNING_RATE,\n",
    "                                     aux_loss = True,\n",
    "                                     sub_criterion = SUB_CRITERION, \n",
    "                                     alpha = ALPHA)\n",
    "        \n",
    "        print('{0}/{1}'.format(ind+1, len(NB_CHANNELS)))\n",
    "        print('With parameters kernel_size/base_channel/nb_hid_neur_inner/nb_hid_neur_out/alpha : {0}/{1}/{2}/{3}/{4}'.format(best_kernel,\n",
    "                                                                                                                              best_ch_size, \n",
    "                                                                                                                              best_fcni,\n",
    "                                                                                                                              n,\n",
    "                                                                                                                              ALPHA))\n",
    "        final_test_loss, final_test_loss_acc = test_losses[-1], test_acc[-1]\n",
    "        print(\"On the test set we obtain a loss of {:.2f} and an accuracy of {:.2f}\".format(final_test_loss,final_test_loss_acc))\n",
    "        \n",
    "        results.append([training_losses, training_acc, test_losses, test_acc])\n",
    "    \n",
    "    \n",
    "    print('round {0} end'.format(i+1))\n",
    "    round_results_nb_neurons_outer.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4, 20)\n",
      "(10, 5)\n",
      "(5,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "np.savez(\"results-batch{0}-nb_fcneurons_outer_search\".format(TRAIN_BATCH_SIZE),round_results_nb_neurons_outer)\n",
    "\n",
    "copy_of = round_results_nb_neurons_outer.copy()\n",
    "\n",
    "print('Shape of \"copy_of\" : {0}'.format(copy_of.shape))\n",
    "\n",
    "last_accs_only = copy_of[:, :, 3, 19]\n",
    "\n",
    "print('Shape of \"last_accs_only\" : {0}'.format(last_accs_only.shape))\n",
    "\n",
    "means_fcno = last_accs_only.mean(axis=0)\n",
    "stds_fcno = last_accs_only.std(axis=0)\n",
    "\n",
    "print(means_fcno)\n",
    "print(stds_fcno)\n",
    "\n",
    "i_max = np.argmax(means_fcno)\n",
    "\n",
    "print(i_max)\n",
    "\n",
    "best_fcno = FCNEURONS[i_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALPHA search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"CUDA available\")\n",
    "else:\n",
    "    print(\"NO CUDA\")\n",
    "\n",
    "rounds = 10\n",
    "round_results_alpha = [] #3D\n",
    "\n",
    "for i in range(rounds):\n",
    "    results = [] #training_losses, training_acc, test_losses, test_acc\n",
    "    \n",
    "    print('round {0} start'.format(i+1))\n",
    "    \n",
    "    for ind, a in enumerate(ALPHAS):\n",
    "        \n",
    "        classifier = MaryJaneNet(base_channel_size = best_ch_size, hidden_layer = best_fcni, kernel_size = best_kernel)\n",
    "\n",
    "        model = LeonardNet(image_net = classifier, hidden_layer = best_fcno)\n",
    "        \n",
    "        training_losses, training_acc, _, _, test_losses, test_acc, test_losses_l, test_losses_r = train_siamese(model = model,\n",
    "                                     dataloader = train_dataloader,\n",
    "                                     test_dataloader = test_dataloader,\n",
    "                                     epochs = EPOCHS,\n",
    "                                     final_criterion = FINAL_CRITERION, \n",
    "                                     learning_rate = LEARNING_RATE,\n",
    "                                     aux_loss = True,\n",
    "                                     sub_criterion = SUB_CRITERION, \n",
    "                                     alpha = a)\n",
    "        \n",
    "        print('{0}/{1}'.format(ind+1, len(NB_CHANNELS)))\n",
    "        print('With parameters kernel_size/base_channel/nb_hid_neur_inner/nb_hid_neur_out/alpha : {0}/{1}/{2}/{3}/{4}'.format(best_kernel,\n",
    "                                                                                                                              best_ch_size, \n",
    "                                                                                                                              best_fcni,\n",
    "                                                                                                                              best_fcno,\n",
    "                                                                                                                              a))\n",
    "        final_test_loss, final_test_loss_acc = test_losses[-1], test_acc[-1]\n",
    "        print(\"On the test set we obtain a loss of {:.2f} and an accuracy of {:.2f}\".format(final_test_loss,final_test_loss_acc))\n",
    "        \n",
    "        results.append([training_losses, training_acc, test_losses, test_acc, test_losses_l, test_losses_r])\n",
    "    \n",
    "    \n",
    "    print('round {0} end'.format(i+1))\n",
    "    round_results_alpha.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4, 20)\n",
      "(10, 5)\n",
      "(5,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "np.savez(\"results-batch{0}-nb_fcneurons_outer_search\".format(TRAIN_BATCH_SIZE),round_results_alpha)\n",
    "\n",
    "copy_of = round_results_alpha.copy()\n",
    "\n",
    "print('Shape of \"copy_of\" : {0}'.format(copy_of.shape))\n",
    "\n",
    "last_accs_only = copy_of[:, :, 3, 19]\n",
    "\n",
    "print('Shape of \"last_accs_only\" : {0}'.format(last_accs_only.shape))\n",
    "\n",
    "means_alpha = last_accs_only.mean(axis=0)\n",
    "stds_alpha = last_accs_only.std(axis=0)\n",
    "\n",
    "print(means_alpha)\n",
    "print(stds_alpha)\n",
    "\n",
    "i_max = np.argmax(means_alpha)\n",
    "\n",
    "print(i_max)\n",
    "\n",
    "best_alpha = ALPHAS[i_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_loss_l_only = copy_of[:, :, 4, 19]\n",
    "last_loss_r_only = copy_of[:, :, 5, 19]\n",
    "\n",
    "means_alpha_loss_l = last_loss_l_only.mean(axis=0)\n",
    "stds_alpha_loss_l = last_loss_l_only.std(axis=0)\n",
    "\n",
    "means_alpha_loss_r = last_loss_r_only.mean(axis=0)\n",
    "stds_alpha_loss_r = last_loss_r_only.std(axis=0)\n",
    "\n",
    "print(means_alpha)\n",
    "print(stds_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
